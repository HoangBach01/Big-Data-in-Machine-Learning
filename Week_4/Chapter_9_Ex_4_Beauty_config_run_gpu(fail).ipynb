{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter_9_Ex_4_Beauty_config_run_gpu.ipynb","provenance":[],"authorship_tag":"ABX9TyOVAFyGAgl1LbHDDLNSUoAE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WXWSaEnsxk2C","executionInfo":{"status":"ok","timestamp":1620727049175,"user_tz":-420,"elapsed":1818,"user":{"displayName":"Tran Bach","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVBXJVq0cKT8rYTGTQdgTlZqt5CNRrP9iVXzZUOw=s64","userId":"16544306707432908367"}},"outputId":"17451d89-5d3d-4585-edb3-94bdabf1773c"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Tue May 11 09:57:28 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JtKWubMHzvqG"},"source":["# getting JDK because Spark is developed using Scala which requires Java runtime environment\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# downloading and unzipping the Spark\n","!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n","!tar -xvf spark-3.1.1-bin-hadoop2.7.tgz\n","\n","# findspark is a utility that automatically set all the os path and initialize the spark context\n","!pip install -q findspark\n","\n","# if you want to use cuda. make sure the version is correct as above\n","!wget https://repo1.maven.org/maven2/ai/rapids/cudf/0.14/cudf-0.14-cuda10-1.jar\n","\n","# rapids for spark is nvidia's framework to train ml models on gpu\n","!wget http://insecure.repo1.maven.org/maven2/com/nvidia/rapids-4-spark_2.12/0.1.0/rapids-4-spark_2.12-0.1.0.jar\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xyplwX4M0wYA","executionInfo":{"status":"ok","timestamp":1620726466979,"user_tz":-420,"elapsed":2348,"user":{"displayName":"Tran Bach","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVBXJVq0cKT8rYTGTQdgTlZqt5CNRrP9iVXzZUOw=s64","userId":"16544306707432908367"}},"outputId":"76c43fbf-c3ce-4649-c21f-25f13139187c"},"source":["# downloading xgboost\n","# !wget https://repo1.maven.org/maven2/com/nvidia/xgboost4j_3.0/1.0.0-0.1.0/xgboost4j_3.0-1.0.0-0.1.0.jar   \n","# !wget https://repo1.maven.org/maven2/com/nvidia/xgboost4j-spark_3.0/1.0.0-0.1.0/xgboost4j-spark_3.0-1.0.0-0.1.0.jar  "],"execution_count":3,"outputs":[{"output_type":"stream","text":["--2021-05-11 09:47:44--  https://repo1.maven.org/maven2/com/nvidia/xgboost4j_3.0/1.0.0-0.1.0/xgboost4j_3.0-1.0.0-0.1.0.jar\n","Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209\n","Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 231556205 (221M) [application/java-archive]\n","Saving to: ‘xgboost4j_3.0-1.0.0-0.1.0.jar’\n","\n","xgboost4j_3.0-1.0.0 100%[===================>] 220.83M   224MB/s    in 1.0s    \n","\n","2021-05-11 09:47:46 (224 MB/s) - ‘xgboost4j_3.0-1.0.0-0.1.0.jar’ saved [231556205/231556205]\n","\n","--2021-05-11 09:47:46--  https://repo1.maven.org/maven2/com/nvidia/xgboost4j-spark_3.0/1.0.0-0.1.0/xgboost4j-spark_3.0-1.0.0-0.1.0.jar\n","Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209\n","Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2040779 (1.9M) [application/java-archive]\n","Saving to: ‘xgboost4j-spark_3.0-1.0.0-0.1.0.jar’\n","\n","xgboost4j-spark_3.0 100%[===================>]   1.95M  --.-KB/s    in 0.03s   \n","\n","2021-05-11 09:47:46 (61.8 MB/s) - ‘xgboost4j-spark_3.0-1.0.0-0.1.0.jar’ saved [2040779/2040779]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9WPvSH3T0LjK","executionInfo":{"status":"ok","timestamp":1620727276649,"user_tz":-420,"elapsed":1139,"user":{"displayName":"Tran Bach","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVBXJVq0cKT8rYTGTQdgTlZqt5CNRrP9iVXzZUOw=s64","userId":"16544306707432908367"}}},"source":["# before doing findspark\n","import os \n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" # set java home\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\" # set spark home. it's the file we downloaded and unpacked just now\n","\n","# transfer all the jar file to the cluster\n","# usually we list this in the command line when we submit spark jobs. \n","# however, since we are working interactively with spark, we set it here\n","os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/cudf-0.14-cuda10-1.jar,/content/xgboost4j_3.0-1.0.0-0.1.0.jar,/content/xgboost4j-spark_3.0-1.0.0-0.1.0.jar,/content/rapids-4-spark_2.12-0.1.0.jar pyspark-shell'"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"utJsZGfw04a7","executionInfo":{"status":"ok","timestamp":1620727186104,"user_tz":-420,"elapsed":988,"user":{"displayName":"Tran Bach","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVBXJVq0cKT8rYTGTQdgTlZqt5CNRrP9iVXzZUOw=s64","userId":"16544306707432908367"}}},"source":["import findspark \n","findspark.init() # go to the spark and java home to initiate the environment"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z4cO-nlD08lw","executionInfo":{"status":"ok","timestamp":1620727232849,"user_tz":-420,"elapsed":30164,"user":{"displayName":"Tran Bach","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVBXJVq0cKT8rYTGTQdgTlZqt5CNRrP9iVXzZUOw=s64","userId":"16544306707432908367"}},"outputId":"57d14a53-b76c-49ec-d8d2-c66685a7e70c"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\", force_remount=True)\n","%cd '/content/gdrive/MyDrive/LDS9_K265_TranHoangBach/Week_4/data_day_7'"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/LDS9_K265_TranHoangBach/Week_4/data_day_7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H91DhWle3oGv","executionInfo":{"status":"ok","timestamp":1620727236087,"user_tz":-420,"elapsed":3234,"user":{"displayName":"Tran Bach","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVBXJVq0cKT8rYTGTQdgTlZqt5CNRrP9iVXzZUOw=s64","userId":"16544306707432908367"}}},"source":["from pyspark import SparkContext\n","from pyspark.conf import SparkConf\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sb\n","import numpy as np\n","import pandas as pd\n","\n","%matplotlib inline"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"LkHsowBQ3qJg","executionInfo":{"status":"ok","timestamp":1620727618046,"user_tz":-420,"elapsed":1287,"user":{"displayName":"Tran Bach","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVBXJVq0cKT8rYTGTQdgTlZqt5CNRrP9iVXzZUOw=s64","userId":"16544306707432908367"}}},"source":["spark = SparkSession.builder.master(\"local[*]\").\\\n","        config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\").\\\n","        config(\"spark.rapids.memory.gpu.pooling.enabled\", False).\\\n","        getOrCreate()\n","# sc = SparkContext(master=\"local\", appName=\"New Spark Context\")\n","# spark = SparkSession(sc)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XSwU67SH3y8h","executionInfo":{"status":"ok","timestamp":1620727697314,"user_tz":-420,"elapsed":76684,"user":{"displayName":"Tran Bach","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVBXJVq0cKT8rYTGTQdgTlZqt5CNRrP9iVXzZUOw=s64","userId":"16544306707432908367"}},"outputId":"e435ba99-6732-46e1-9be1-0743d378aafb"},"source":["df = spark.read.csv(\"ratings_Beauty.csv\", header=False, inferSchema=True)\n","df = df.withColumnRenamed('_c1', 'product_id')\n","df = df.withColumnRenamed('_c0', 'user_id')\n","df = df.withColumnRenamed('_c2', 'label')\n","df = df.select('product_id', 'user_id', 'label')\n","df.show(5)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["+----------+--------------+-----+\n","|product_id|       user_id|label|\n","+----------+--------------+-----+\n","|0205616461|A39HTATAQ9V7YF|  5.0|\n","|0558925278|A3JM6GV9MNOF9X|  3.0|\n","|0558925278|A1Z513UWSAAO0F|  5.0|\n","|0733001998|A1WMRR494NWEWV|  4.0|\n","|0737104473|A3IAAVS479H7M7|  1.0|\n","+----------+--------------+-----+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aLal9NyO31bd","executionInfo":{"status":"ok","timestamp":1620727985249,"user_tz":-420,"elapsed":278533,"user":{"displayName":"Tran Bach","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVBXJVq0cKT8rYTGTQdgTlZqt5CNRrP9iVXzZUOw=s64","userId":"16544306707432908367"}},"outputId":"c96fd4b1-2d5f-40f6-e120-9b664266eb5f"},"source":["from pyspark.ml.feature import StringIndexer\n","from pyspark.ml.pipeline import Pipeline\n","\n","indexer_product = StringIndexer(inputCol='product_id', outputCol='product_idx')\n","indexer_user = StringIndexer(inputCol='user_id', outputCol='user_idx')\n","\n","pre_pipeline = Pipeline(stages=[indexer_product, indexer_user])\n","pre_pipeline_fitted = pre_pipeline.fit(df)\n","final_df = pre_pipeline_fitted.transform(df)\n","\n","final_df.show(5)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["+----------+--------------+-----+-----------+--------+\n","|product_id|       user_id|label|product_idx|user_idx|\n","+----------+--------------+-----+-----------+--------+\n","|0205616461|A39HTATAQ9V7YF|  5.0|   145790.0| 70392.0|\n","|0558925278|A3JM6GV9MNOF9X|  3.0|   103581.0|265306.0|\n","|0558925278|A1Z513UWSAAO0F|  5.0|   103581.0|552933.0|\n","|0733001998|A1WMRR494NWEWV|  4.0|   145791.0|536779.0|\n","|0737104473|A3IAAVS479H7M7|  1.0|   145792.0| 14679.0|\n","+----------+--------------+-----+-----------+--------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qRe9iF7t39jx","executionInfo":{"status":"ok","timestamp":1620727985250,"user_tz":-420,"elapsed":238578,"user":{"displayName":"Tran Bach","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVBXJVq0cKT8rYTGTQdgTlZqt5CNRrP9iVXzZUOw=s64","userId":"16544306707432908367"}}},"source":["train_df, test_df = final_df.randomSplit([0.9, 0.1], seed=42)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"13P34liX3_vA"},"source":["from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.ml.recommendation import ALS\n","\n","als = ALS(maxIter=10,           # Number of iterations\n","          regParam=0.1,        # Regularization parameter beta\n","          rank=20,              # Number of features\n","          numItemBlocks=10,     # Number partitioned to parallelize computation\n","          alpha=0.001,            # Learning rate\n","          userCol='user_idx',     \n","          itemCol='product_idx',\n","          ratingCol='label')\n","model = als.fit(train_df)\n","\n","import time\n","tic = time.time()\n","predictions = model.transform(test_df)\n","predictions.show(5)\n","evaluator = RegressionEvaluator(metricName='rmse')\n","rmse = evaluator.evaluate(predictions)\n","print('RMSE: {:.4f}'.format(rmse))\n","\n","toc = time.time()\n","print('Total time: {:.2f} seconds'.format(toc-tic))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TDg_ezLy4Fi8"},"source":[""],"execution_count":null,"outputs":[]}]}